{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "44b4b74f",
      "metadata": {
        "id": "44b4b74f"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "23d0372c",
      "metadata": {
        "id": "23d0372c"
      },
      "outputs": [],
      "source": [
        "# Remove unwanted warnings\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "warnings.simplefilter(action='ignore', category=RuntimeWarning)\n",
        "\n",
        "# Data Management\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pandas_datareader.data import DataReader\n",
        "from ta import add_all_ta_features\n",
        "import os\n",
        "\n",
        "# Statistics\n",
        "from statsmodels.tsa.stattools import adfuller\n",
        "\n",
        "# Unsupervised Machine Learning\n",
        "from sklearn.decomposition import PCA\n",
        "import joblib\n",
        "\n",
        "# Supervised Machine Learning\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import precision_score\n",
        "import os\n",
        "# Reporting\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "339f3702",
      "metadata": {
        "id": "339f3702"
      },
      "source": [
        "### Initial Data Extraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dbaff695-0b08-468f-b5d2-36c3ee7f120a",
      "metadata": {
        "id": "dbaff695-0b08-468f-b5d2-36c3ee7f120a"
      },
      "outputs": [],
      "source": [
        "# Set the directory where the new .pk1 file is saved\n",
        "augmented_directory = 'Data Augmented'\n",
        "# Set the file name of the new .pk1 file\n",
        "file_name = 'XAU_USD_M15_2024.pk1'\n",
        "# Construct the full path to the .pk1 file\n",
        "file_path = os.path.join(augmented_directory, file_name)\n",
        "\n",
        "# Check if the file exists\n",
        "if os.path.exists(file_path):\n",
        "    # Load the DataFrame from the pickle file\n",
        "    df = pd.read_pickle(file_path)\n",
        "\n",
        "    # Generate a list of columns to drop that contain 'ask' or 'bid'\n",
        "    cols_to_drop = [col for col in df.columns if 'ask' in col or 'bid' in col]\n",
        "\n",
        "    # Drop the columns from the DataFrame\n",
        "    df.drop(cols_to_drop, axis=1, inplace=True)\n",
        "\n",
        "        # Add TA features to the DataFrame\n",
        "    # Ensure df has the columns: 'mid_o', 'mid_h', 'mid_l', 'mid_c', 'volume'\n",
        "    df = add_all_ta_features(\n",
        "        df,\n",
        "        open=\"mid_o\", high=\"mid_h\", low=\"mid_l\", close=\"mid_c\", volume=\"volume\",\n",
        "        fillna=True\n",
        "    )\n",
        "\n",
        "    #Convert the 'Time' column to datetime (this step may not be necessary if it's already in datetime format)\n",
        "    df['time'] = pd.to_datetime(df['time'])\n",
        "\n",
        "    # Set the 'Time' column as the index of the DataFrame\n",
        "    df = df.set_index('time')\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "798a8d07-f6ab-4c86-968d-662874b1d86c",
      "metadata": {
        "id": "798a8d07-f6ab-4c86-968d-662874b1d86c",
        "outputId": "d313c8b0-e644-48d4-a7e4-4e4566f9760f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The first few rows of the DataFrame:\n",
            "                       time  volume     mid_o     mid_h     mid_l     mid_c  \\\n",
            "0 2024-01-01 23:00:00+00:00     397  2065.845  2068.995  2064.135  2065.865   \n",
            "1 2024-01-01 23:15:00+00:00     498  2065.870  2066.295  2065.235  2065.275   \n",
            "2 2024-01-01 23:30:00+00:00     457  2065.225  2065.535  2064.300  2064.620   \n",
            "3 2024-01-01 23:45:00+00:00     554  2064.560  2064.590  2063.250  2063.795   \n",
            "4 2024-01-02 00:00:00+00:00     488  2063.835  2065.035  2063.475  2064.105   \n",
            "\n",
            "     bid_o     bid_h    bid_l    bid_c    ask_o     ask_h    ask_l    ask_c  \n",
            "0  2064.40  2066.495  2063.04  2065.62  2067.29  2071.495  2064.62  2066.11  \n",
            "1  2065.63  2066.080  2065.01  2065.07  2066.11  2066.540  2065.43  2065.48  \n",
            "2  2065.07  2065.360  2064.14  2064.46  2065.38  2065.770  2064.46  2064.78  \n",
            "3  2064.40  2064.430  2063.10  2063.62  2064.72  2064.750  2063.40  2063.97  \n",
            "4  2063.67  2064.880  2063.31  2063.95  2064.00  2065.190  2063.64  2064.26  \n",
            "\n",
            "The last few rows of the DataFrame:\n",
            "                          time  volume     mid_o     mid_h     mid_l  \\\n",
            "2933 2024-02-14 21:45:00+00:00     592  1991.880  1993.020  1991.710   \n",
            "2934 2024-02-14 23:00:00+00:00     208  1993.305  1993.945  1992.770   \n",
            "2935 2024-02-14 23:15:00+00:00     165  1993.285  1993.420  1992.915   \n",
            "2936 2024-02-14 23:30:00+00:00     179  1993.005  1993.680  1992.865   \n",
            "2937 2024-02-14 23:45:00+00:00     363  1993.380  1993.905  1993.125   \n",
            "\n",
            "         mid_c    bid_o    bid_h    bid_l    bid_c    ask_o    ask_h    ask_l  \\\n",
            "2933  1992.365  1991.66  1992.81  1991.47  1992.11  1992.10  1993.23  1991.93   \n",
            "2934  1993.350  1992.09  1993.37  1992.09  1993.19  1994.52  1994.52  1992.96   \n",
            "2935  1993.010  1993.16  1993.26  1992.78  1992.86  1993.41  1993.63  1993.05   \n",
            "2936  1993.395  1992.85  1993.53  1992.72  1993.23  1993.16  1993.83  1993.01   \n",
            "2937  1993.800  1993.22  1993.75  1992.96  1993.64  1993.54  1994.08  1993.29   \n",
            "\n",
            "        ask_c  \n",
            "2933  1992.62  \n",
            "2934  1993.51  \n",
            "2935  1993.16  \n",
            "2936  1993.56  \n",
            "2937  1993.96  \n"
          ]
        }
      ],
      "source": [
        "# Check if the file exists and load it\n",
        "if os.path.exists(file_path):\n",
        "    df = pd.read_pickle(file_path)\n",
        "\n",
        "    # Display the first few rows of the DataFrame\n",
        "    print(\"The first few rows of the DataFrame:\")\n",
        "    print(df.head())\n",
        "\n",
        "    # Display the last few rows of the DataFrame\n",
        "    print(\"\\nThe last few rows of the DataFrame:\")\n",
        "    print(df.tail())\n",
        "else:\n",
        "    print(f\"The file does not exist at the specified path: {file_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "46f532a5",
      "metadata": {
        "id": "46f532a5"
      },
      "source": [
        "### Data Preprocessing - Target Setting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4f08a600-5864-4e25-b163-443704df1a25",
      "metadata": {
        "id": "4f08a600-5864-4e25-b163-443704df1a25"
      },
      "outputs": [],
      "source": [
        "# Set initial signal based on whether the next close is higher or lower than the current close\n",
        "df[\"signal\"] = 1  # Default to sell\n",
        "df.loc[df[\"mid_c\"].shift(-1) > df[\"mid_c\"], \"signal\"] = 2  # Change to buy if next close is higher\n",
        "\n",
        "# For sell positions, if the distance to next mid_h is greater than the distance to next mid_c, set signal to 0\n",
        "df.loc[(df[\"signal\"] == 1) & (abs(df[\"mid_h\"].shift(-1) - df[\"mid_c\"]) > abs(df[\"mid_c\"].shift(-1) - df[\"mid_c\"])), \"signal\"] = 0\n",
        "\n",
        "# For buy positions, if the distance to next mid_l is greater than the distance to next mid_c, set signal to 0\n",
        "df.loc[(df[\"signal\"] == 2) & (abs(df[\"mid_l\"].shift(-1) - df[\"mid_c\"]) > abs(df[\"mid_c\"].shift(-1) - df[\"mid_c\"])), \"signal\"] = 0\n",
        "\n",
        "# Remove rows with NaN values resulting from the shift operation\n",
        "df.dropna(inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "453652c4-f06e-432e-8640-9709210e6186",
      "metadata": {
        "id": "453652c4-f06e-432e-8640-9709210e6186"
      },
      "outputs": [],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2a0b2c5d",
      "metadata": {
        "id": "2a0b2c5d"
      },
      "outputs": [],
      "source": [
        "# Split Target from Featureset\n",
        "x = df.iloc[:, :-1]\n",
        "y = df.iloc[:, -1]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3d287d20",
      "metadata": {
        "id": "3d287d20"
      },
      "source": [
        "### Data Preprocessing - Stationarity and Scaling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "306d5a74",
      "metadata": {
        "id": "306d5a74"
      },
      "outputs": [],
      "source": [
        "# Identify non-stationary columns\n",
        "non_stationaries = []\n",
        "for col in x.columns:\n",
        "    # Perform Augmented Dickey-Fuller test only on numeric columns\n",
        "    if x[col].dtype == 'float64' or x[col].dtype == 'int64':\n",
        "        result = adfuller(x[col].dropna())  # Drop NA values as ADF doesn't handle them\n",
        "        p_value = result[1]\n",
        "        test_statistic = result[0]\n",
        "        critical_value = result[4][\"1%\"]\n",
        "\n",
        "        # Check if p-value is above 0.05 or test statistic is higher than critical value\n",
        "        if p_value > 0.05 or test_statistic > critical_value:\n",
        "            non_stationaries.append(col)\n",
        "\n",
        "print(f\"Non-Stationary Features Found: {len(non_stationaries)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2ca5bc27",
      "metadata": {
        "id": "2ca5bc27"
      },
      "outputs": [],
      "source": [
        "# Convert non-stationaries to stationary\n",
        "df_stationary = x.copy()\n",
        "df_stationary[non_stationaries] = df_stationary[non_stationaries].pct_change()\n",
        "df_stationary = df_stationary.iloc[1:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "53b760a8",
      "metadata": {
        "id": "53b760a8"
      },
      "outputs": [],
      "source": [
        "# Find NaN Rows\n",
        "na_list = df_stationary.columns[df_stationary.isna().any().tolist()]\n",
        "df_stationary.drop(columns=na_list, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5882ede1",
      "metadata": {
        "id": "5882ede1"
      },
      "outputs": [],
      "source": [
        "# Handle inf values\n",
        "df_stationary.replace([np.inf, -np.inf], 0, inplace=True)\n",
        "df_stationary.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "678f80f5",
      "metadata": {
        "scrolled": true,
        "id": "678f80f5"
      },
      "outputs": [],
      "source": [
        "# Feature Scaling\n",
        "df_sc = df_stationary.copy()\n",
        "X_fs = StandardScaler().fit_transform(df_sc)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "152ffbef",
      "metadata": {
        "id": "152ffbef"
      },
      "source": [
        "### Unsupervised ML - PCA Dimensionality Reduction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1a87da28",
      "metadata": {
        "id": "1a87da28"
      },
      "outputs": [],
      "source": [
        "# Initialize PCA with the desired number of components\n",
        "n_components = 26\n",
        "pca = PCA(n_components=n_components)\n",
        "\n",
        "# Fit PCA on the scaled feature set 'X_fs' and transform the data\n",
        "X_pca = pca.fit_transform(X_fs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "10a9a987",
      "metadata": {
        "id": "10a9a987"
      },
      "outputs": [],
      "source": [
        "# Calculate the variance explained by Principle Components\n",
        "print(\"Variance of each component: \", pca.explained_variance_ratio_)\n",
        "print(\"\\n Total Variance Explained: \", round(sum(list(pca.explained_variance_ratio_)) * 100, 2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c4b87f9a-71fd-4f2c-b524-8580e1ce6ee0",
      "metadata": {
        "id": "c4b87f9a-71fd-4f2c-b524-8580e1ce6ee0"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Assuming X_fs are your scaled features\n",
        "\n",
        "# Fit PCA on the entire dataset to include all components\n",
        "pca = PCA()\n",
        "pca.fit(X_fs)\n",
        "\n",
        "# Calculate the cumulative sum of explained variance ratio\n",
        "cumulative_variance = pca.explained_variance_ratio_.cumsum()\n",
        "\n",
        "# Plotting the scree plot\n",
        "plt.figure(figsize=(10, 7))\n",
        "plt.bar(range(1, len(pca.explained_variance_ratio_) + 1), pca.explained_variance_ratio_, alpha=0.5, align='center', label='Individual explained variance')\n",
        "plt.step(range(1, len(cumulative_variance) + 1), cumulative_variance, where='mid', label='Cumulative explained variance')\n",
        "\n",
        "# Adding labels and title to the plot\n",
        "plt.xlabel('Principal Component')\n",
        "plt.ylabel('Explained Variance Ratio')\n",
        "plt.title('Scree Plot')\n",
        "plt.legend(loc='best')\n",
        "\n",
        "# Show grid\n",
        "plt.grid()\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5d715175",
      "metadata": {
        "id": "5d715175"
      },
      "outputs": [],
      "source": [
        "# Create columns\n",
        "pca_cols = []\n",
        "for i in range(n_components):\n",
        "    pca_cols.append(f\"PC_{i}\")\n",
        "pca_cols"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a1d1ea9d",
      "metadata": {
        "id": "a1d1ea9d"
      },
      "outputs": [],
      "source": [
        "# Create and View DataFrame\n",
        "\n",
        "df_pca = pd.DataFrame(data=X_pca, columns=pca_cols)\n",
        "\n",
        "# Display the first few rows of the DataFrame\n",
        "df_pca.head()\n",
        "df_pca.tail()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5d7c568b-1e65-4514-b927-bd38574eff78",
      "metadata": {
        "id": "5d7c568b-1e65-4514-b927-bd38574eff78"
      },
      "outputs": [],
      "source": [
        "# Reset index if needed (do this for both 'df_pca' and 'y' if their indices do not match)\n",
        "df_pca.reset_index(drop=True, inplace=True)\n",
        "y.reset_index(drop=True, inplace=True)\n",
        "\n",
        "# Now add the target variable to the PCA DataFrame\n",
        "df_pca['signal'] = y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e1590895-c661-443c-823d-bc6420ec41f6",
      "metadata": {
        "id": "e1590895-c661-443c-823d-bc6420ec41f6"
      },
      "outputs": [],
      "source": [
        "print(df_pca.head())\n",
        "print(df_pca.tail())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "49c89dfb-cf5b-4268-96b4-f0753b2e43fe",
      "metadata": {
        "id": "49c89dfb-cf5b-4268-96b4-f0753b2e43fe"
      },
      "outputs": [],
      "source": [
        "# Define the directory path\n",
        "directory_path = 'Data Augmented'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9cb515a7-91c3-4de3-bde5-88dfdd26fc20",
      "metadata": {
        "id": "9cb515a7-91c3-4de3-bde5-88dfdd26fc20"
      },
      "outputs": [],
      "source": [
        "# Define the full path for the pickle file including the directory and file name\n",
        "file_path = os.path.join(directory_path, 'XAU_USD_M15_AUGMENTED_MULTICLASS.pkl')\n",
        "\n",
        "# Save the DataFrame to a pickle file in the specified directory\n",
        "try:\n",
        "    df_pca.to_pickle(file_path)\n",
        "    print(f\"DataFrame saved as a pickle file at: {file_path}\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred while saving the DataFrame: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bdc94fad-e97f-496f-a276-6608aa269ace",
      "metadata": {
        "id": "bdc94fad-e97f-496f-a276-6608aa269ace"
      },
      "outputs": [],
      "source": [
        "# Read the pickle file\n",
        "try:\n",
        "    df_from_pickle = pd.read_pickle(file_path)\n",
        "    print(\"First few rows of the DataFrame:\")\n",
        "    print(df_from_pickle.head())  # Display the first few rows of the DataFrame\n",
        "    print(\"\\nLast few rows of the DataFrame:\")\n",
        "    print(df_from_pickle.tail())  # Display the last few rows of the DataFrame\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred while loading the DataFrame: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a2618461-5d0a-4c0c-b304-5a9ec38014c8",
      "metadata": {
        "id": "a2618461-5d0a-4c0c-b304-5a9ec38014c8"
      },
      "outputs": [],
      "source": [
        "# Define the path to the pickle file\n",
        "pickle_file_path = os.path.join('Data Augmented', 'XAU_USD_M15_AUGMENTED_MULTICLASS.pkl')\n",
        "\n",
        "# Read the DataFrame from the pickle file\n",
        "try:\n",
        "    df = pd.read_pickle(pickle_file_path)\n",
        "    print(\"DataFrame loaded successfully from the pickle file.\")\n",
        "    # Display the first few rows to verify\n",
        "    print(df.head())\n",
        "    # Display the last few rows to verify\n",
        "    print(df.tail())\n",
        "except FileNotFoundError:\n",
        "    print(f\"The file {pickle_file_path} was not found.\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "01e915e6-68ab-4f30-a43a-a053916ac279",
      "metadata": {
        "id": "01e915e6-68ab-4f30-a43a-a053916ac279"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}